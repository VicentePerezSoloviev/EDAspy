Metadata-Version: 2.1
Name: EDAspy
Version: 1.1.1
Summary: EDAspy is a Python package that implements Estimation of Distribution Algorithms. EDAspy allows toeither use already existing implementations or customize the EDAs baseline easily building it bymodules so new research can be easily developed. It also has several benchmarks for comparisons.
Home-page: https://github.com/VicentePerezSoloviev/EDAspy
Author: Vicente P. Soloviev
Author-email: vicente.perez.soloviev@gmail.com
License: MIT
Download-URL: https://github.com/VicentePerezSoloviev/EDAspy/archive/1.1.1.tar.gz
Description: # <img src='https://raw.githubusercontent.com/VicentePerezSoloviev/EDAspy/master/Logo%20EDAspy.png' align="right" height="150"/>
        
        [![PyPI](https://img.shields.io/pypi/v/edaspy)](https://pypi.python.org/pypi/EDAspy/)
        [![PyPI license](https://img.shields.io/pypi/l/EDAspy.svg)](https://pypi.python.org/pypi/EDAspy/)
        [![Downloads](https://static.pepy.tech/personalized-badge/edaspy?period=total&units=none&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/edaspy)
        [![Documentation Status](https://readthedocs.org/projects/edaspy/badge/?version=latest)](https://edaspy.readthedocs.io/en/latest/?badge=latest)
        
        # EDAspy
        
        ## Introduction
        
        EDAspy presents some implementations of the Estimation of Distribution Algorithms (EDAs) [1]. EDAs are a type of
        evolutionary algorithms. Depending on the type of the probabilistic model embedded in the EDA, and the type of
        variables considered, we will use a different EDA implementation.
        
        The pseudocode of EDAs is the following:
        
        1. Random initialization of the population.
        
        2. Evaluate each individual of the population.
        
        3. Select the top best individuals according to cost function evaluation.
        
        4. Learn a probabilistic model from the best individuals selected.
        
        5. Sampled another population.
        
        6. If stopping criteria is met, finish; else, go to 2.
        
        EDAspy allows to create a custom version of the EDA. Using the modular probabilistic models and the initializators, this can be embedded into the EDA baseline and used for different purposes. If this fits you, take a look on the examples section to the EDACustom example.
        
        EDAspy also incorporates a set of benchmarks in order to compare the algorithms trying to minimize these cost functions.
        
        The following implementations are available in EDAspy:
        
        * UMDAd: Univariate Marginal Distribution Algorithm binary [2]. It can be used as a simple example of EDA where the variables are binary and there are not dependencies between variables. Some usages include feature selection, for example.
        
        
        * UMDAc: Univariate Marginal Distribution Algorithm continuous [3]. In this EDA all the variables assume a Gaussian distribution and there are not dependencies considered between the variables. Some usages include hyperparameter optimization, for example.
        
        
        * UnivariateKEDA: Univariate Kernel Estimation of Distribution Algorithm [4]. Each variables distribution is estimated using Kernel Density Estimation. 
        
        
        * EGNA: Estimation of Gaussian Distribution Algorithm [5][6]. This is a complex implementation in which dependencies between the variables are considered during the optimization. In each iteration, a Gaussian Bayesian network is learned and sampled. The variables in the model are assumed to be Gaussian and also de dependencies between them. This implementation is focused in continuous optimization.
        
        
        * EMNA: Estimation of Multivariate Normal Algorithm [1]. This is a similar implementation to EGNA, in which instead of using a Gaussian Bayesian network, a multivariate Gaussian distribution is iteratively learned and sampled. As in EGNA, the dependencies between variables are considered and assumed to be linear Gaussian. This implementation is focused in continuous optimization.
        
        
        * SPEDA: Semiparametric Estimation of Distribution Algorithm [7]. This multivariate EDA allows estimating the density of a variable using either KDE or Gaussians, and allow dependencies between both types of variables. It is an archive-based approach where the probabilistic model is updated given the best individuals of l previous generations.
        
        
        * MultivariateKEDA: Special case of SPEDA approach in which all nodes are restricted to be estimated using KDE (Gaussian nodes are forbidden) [7]. It is also an archive-based approach.
        
        
        * Categorical EDA. In this implementation we consider some independent categorical variables. Some usages include portfolio optimization, for exampled.
        
        
        Some tools are also available in EDAspy such as the Bayesian network structure plotting, for visualizing the graph learnt in some of the implementations, if needed.
        
        
        Although some categorical EDAs are implemented, the package is focused in continuous optimization. Below, we show a CPU time analysis for the different approaches implemented for continuous optimization. Note that the CPU time can be reduced using parallelization (available as a parameter in the EDA initialization). Reference [7] shows a comparison about the performance of the algorithms in terms of cost function minimization. 
        
        <img src='cpu_comparison_continuous_opt.jpeg' alt="CPU time comparison for continuous optimization" title="CPU time comparison for continuous optimization"/>
        
        ## Examples
        
        Some examples are available in https://github.com/VicentePerezSoloviev/EDAspy/tree/master/notebooks
        
        ## Getting started
        
        For installing EDAspy from Pypi execute the following command using pip:
        
        ```bash
            pip install EDAspy
        ```
        
        ## Build from Source
        
        ### Prerequisites
        
        - Python >= 3.0
        - Pybnesian, numpy, pandas.
        
        ### Building
        
        Clone the repository:
        
        ```bash
            git clone https://github.com/VicentePerezSoloviev/EDAspy.git
            cd EDAspy
            git checkout v1.0.0 # You can checkout a specific version if you want
            python setup.py install
        ```
        ## Testing 
        
        The library contains tests that can be executed using `pytest <https://docs.pytest.org/>`_. Install it using 
        pip:
        
        ```bash
          pip install pytest
        ```
        
        Run the tests with:
        
        ```bash
          pytest
        ```
        
        ## Bibliography
        
        [1] LarraÃ±aga, P., & Lozano, J. A. (Eds.). (2001). Estimation of distribution algorithms: A new tool for evolutionary computation (Vol. 2). Springer Science & Business Media.
        
        [2] MÃ¼hlenbein, H., & Paass, G. (1996). From recombination of genes to the estimation of distributions I. Binary parameters. In Parallel Problem Solving from Natureâ€”PPSN IV: International Conference on Evolutionary Computationâ€”The 4th International Conference on Parallel Problem Solving from Nature Berlin, Germany, September 22â€“26, 1996 Proceedings 4 (pp. 178-187). Springer Berlin Heidelberg.
        
        [3] MÃ¼hlenbein, H., Bendisch, J., & Voigt, H. M. (1996). From recombination of genes to the estimation of distributions II. Continuous parameters. In Parallel Problem Solving from Natureâ€”PPSN IV: International Conference on Evolutionary Computationâ€”The 4th International Conference on Parallel Problem Solving from Nature Berlin, Germany, September 22â€“26, 1996 Proceedings 4 (pp. 188-197). Springer Berlin Heidelberg.
        
        [4] Luo, N., & Qian, F. (2009, August). Evolutionary algorithm using kernel density estimation model in continuous domain. In 2009 7th Asian Control Conference (pp. 1526-1531). IEEE.
        
        [5] Larranaga, P. (2000). Optimization in continuous domains by learning and simulation of Gaussian networks. In Proc. of the 2000 Genetic and Evolutionary Computation Conference Workshop Program.
        
        [6] Soloviev, V. P., LarraÃ±aga, P., & Bielza, C. (2022). Estimation of distribution algorithms using Gaussian Bayesian networks to solve industrial optimization problems constrained by environment variables. Journal of Combinatorial Optimization, 44(2), 1077-1098.
        
        [7] Soloviev, Vicente P.& Bielza, Concha & LarraÃ±aga, Pedro (2023). Semiparametric Estimation of Distribution Algorithms for continuous optimization. IEEE Transactions on Evolutionary Computation.
        
        
Keywords: EDA,estimation,bayesian,evolutionary,algorithm,optimization,time_series,feature,selection,semiparametric,Gaussian
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.0
Description-Content-Type: text/markdown
